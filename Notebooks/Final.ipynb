{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9295456-2cd1-4d73-846a-b9a8b1fa8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define years for interpolation\n",
    "years = [1990, 2019, 2021]\n",
    "\n",
    "# Country-wise Parkinson's disease deaths (in thousands), estimated\n",
    "china_deaths = [20, 90, 115]\n",
    "india_deaths = [10, 40, 50]\n",
    "europe_deaths = [30, 100, 120]\n",
    "us_deaths = [15, 110, 130]\n",
    "germany_deaths = [5, 30, 35]\n",
    "brazil_deaths = [2, 15, 18]\n",
    "japan_deaths = [3, 20, 25]\n",
    "uk_deaths = [3, 20, 22]\n",
    "russia_deaths = [5, 40, 45]\n",
    "\n",
    "# Interpolate from 1990 to 2024\n",
    "full_years = np.arange(1990, 2025, 1)\n",
    "china_interp = np.interp(full_years, years, china_deaths)\n",
    "india_interp = np.interp(full_years, years, india_deaths)\n",
    "europe_interp = np.interp(full_years, years, europe_deaths)\n",
    "us_interp = np.interp(full_years, years, us_deaths)\n",
    "germany_interp = np.interp(full_years, years, germany_deaths)\n",
    "brazil_interp = np.interp(full_years, years, brazil_deaths)\n",
    "japan_interp = np.interp(full_years, years, japan_deaths)\n",
    "uk_interp = np.interp(full_years, years, uk_deaths)\n",
    "russia_interp = np.interp(full_years, years, russia_deaths)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(full_years, china_interp, label='China', color='green')\n",
    "plt.plot(full_years, india_interp, label='India', color='red')\n",
    "plt.plot(full_years, europe_interp, label='Europe', color='purple')\n",
    "plt.plot(full_years, us_interp, label='US', color='orange')\n",
    "plt.plot(full_years, germany_interp, label='Germany', color='brown')\n",
    "plt.plot(full_years, brazil_interp, label='Brazil', color='cyan')\n",
    "plt.plot(full_years, japan_interp, label='Japan', color='magenta')\n",
    "plt.plot(full_years, uk_interp, label='UK', color='pink')\n",
    "plt.plot(full_years, russia_interp, label='Russia', color='yellow')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Deaths (thousands)')\n",
    "plt.title(\"Parkinson's Disease Deaths by Country (1990–2024)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e90d0-a1ec-4599-beb4-486ee3c6c936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdbfixer import PDBFixer\n",
    "from openmm.app import PDBFile\n",
    "import sys\n",
    "\n",
    "# --- Step 1: Repair the PDB File ---\n",
    "fixer = PDBFixer(filename=\"5caw.pdb\")\n",
    "fixer.findMissingResidues()\n",
    "fixer.findMissingAtoms()\n",
    "fixer.addMissingAtoms()\n",
    "\n",
    "# Add hydrogens at a given pH (this may help with aromaticity perception)\n",
    "fixer.addMissingHydrogens(pH=7.0)\n",
    "\n",
    "output_pdb = \"repaired.pdb\"\n",
    "with open(output_pdb, \"w\") as f:\n",
    "    PDBFile.writeFile(fixer.topology, fixer.positions, f)\n",
    "print(\"Repaired PDB file written to:\", output_pdb)\n",
    "\n",
    "# --- Step 2: Convert PDB to PDBQT via Command-Line Open Babel ---\n",
    "import subprocess\n",
    "try:\n",
    "    subprocess.run(\n",
    "        'obabel repaired.pdb -O repaired.pdbqt --addhydrogens --partialcharge gasteiger',\n",
    "        shell=True,\n",
    "        check=True\n",
    "    )\n",
    "    print(\"Converted PDBQT file written to: repaired.pdbqt\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(\"Error during Open Babel conversion:\", e)\n",
    "    sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f470d21-d118-41bf-8707-e241ded29fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "# Load molecules from the SDF file.\n",
    "supplier = Chem.SDMolSupplier(\"UBR_box.sdf\")\n",
    "if not supplier:\n",
    "    raise IOError(\"Could not open compounds.sdf\")\n",
    "\n",
    "# Iterate over each molecule and write it to an individual SDF file.\n",
    "for idx, mol in enumerate(supplier):\n",
    "    if mol is None:\n",
    "        print(f\"Skipping molecule at index {idx} (failed to parse)\")\n",
    "        continue\n",
    "    # Create a writer for the current molecule.\n",
    "    writer = Chem.SDWriter(f\"UBR_box_{idx}.sdf\")\n",
    "    writer.write(mol)\n",
    "    writer.close()\n",
    "    print(f\"Saved compound_{idx}.sdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6dc3a-c9d3-44b5-9278-fbb4b53f63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import subprocess\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, Descriptors, Lipinski\n",
    "\n",
    "###############################\n",
    "# Configuration\n",
    "###############################\n",
    "# Folder containing raw training compounds in PDB format\n",
    "INPUT_FOLDER = \"compounds\"  \n",
    "\n",
    "# Receptor file in PDBQT format\n",
    "PROTEIN_PDBQT = \"5cawrepair.pdbqt\"\n",
    "\n",
    "# Output folder: filtered compounds that pass docking criteria\n",
    "OUTPUT_FOLDER = \"training_compounds_filtered\"\n",
    "\n",
    "# Checkpoint file to track processed files\n",
    "CHECKPOINT_FILE = \"docking_training_checkpoint.json\"\n",
    "\n",
    "# Docking parameters\n",
    "DOCKING_CENTER = [-12.646, 5.094, -59.525]   # Adjust based on your binding site\n",
    "DOCKING_SIZE = [30, 30, 30]                    # Dimensions of the docking box\n",
    "AFFINITY_CUTOFF = -7.0                         # Only compounds with docking score < -7.0 are accepted\n",
    "\n",
    "# Create necessary folders if they don't exist\n",
    "for folder in [OUTPUT_FOLDER, \"temp_files\"]:\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "TEMP_FOLDER = \"temp_files\"\n",
    "\n",
    "###############################\n",
    "# Checkpoint Functions\n",
    "###############################\n",
    "def load_checkpoint():\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {\"processed\": []}\n",
    "\n",
    "def save_checkpoint(data):\n",
    "    with open(CHECKPOINT_FILE, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "###############################\n",
    "# Lipinski Rule of 5 Check\n",
    "###############################\n",
    "def passes_lipinski_ro5(mol):\n",
    "    try:\n",
    "        mw = Descriptors.MolWt(mol)\n",
    "        logp = Descriptors.MolLogP(mol)\n",
    "        hbd = Lipinski.NumHDonors(mol)\n",
    "        hba = Lipinski.NumHAcceptors(mol)\n",
    "        return (mw <= 500 and logp <= 5 and hbd <= 5 and hba <= 10)\n",
    "    except Exception as e:\n",
    "        print(\"Error in Lipinski check:\", e)\n",
    "        return False\n",
    "\n",
    "###############################\n",
    "# Conversion Function: MOL -> PDBQT\n",
    "###############################\n",
    "def convert_to_pdbqt(mol, pdbqt_path):\n",
    "    temp_pdb = os.path.join(TEMP_FOLDER, \"temp.pdb\")\n",
    "    Chem.MolToPDBFile(mol, temp_pdb)\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\"obabel\", temp_pdb, \"-O\", pdbqt_path, \"--partialcharge\", \"gasteiger\"],\n",
    "            check=True,\n",
    "            capture_output=True\n",
    "        )\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error converting to PDBQT for {pdbqt_path}: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        if os.path.exists(temp_pdb):\n",
    "            os.remove(temp_pdb)\n",
    "\n",
    "###############################\n",
    "# Docking Function using AutoDock Vina\n",
    "###############################\n",
    "def run_vina_docking(pdbqt_path):\n",
    "    output_file = pdbqt_path.replace(\".pdbqt\", \"_out.pdbqt\")\n",
    "    try:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"vina\",\n",
    "                \"--receptor\", PROTEIN_PDBQT,\n",
    "                \"--ligand\", pdbqt_path,\n",
    "                \"--center_x\", str(DOCKING_CENTER[0]),\n",
    "                \"--center_y\", str(DOCKING_CENTER[1]),\n",
    "                \"--center_z\", str(DOCKING_CENTER[2]),\n",
    "                \"--size_x\", str(DOCKING_SIZE[0]),\n",
    "                \"--size_y\", str(DOCKING_SIZE[1]),\n",
    "                \"--size_z\", str(DOCKING_SIZE[2]),\n",
    "                \"--exhaustiveness\", \"8\",\n",
    "                \"--out\", output_file\n",
    "            ],\n",
    "            check=True,\n",
    "            capture_output=True\n",
    "        )\n",
    "        with open(output_file, 'r') as f:\n",
    "            for line in f:\n",
    "                if \"REMARK VINA RESULT\" in line:\n",
    "                    return float(line.split()[3])\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Docking failed for {pdbqt_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "###############################\n",
    "# Main Workflow: Docking and Filtering\n",
    "###############################\n",
    "def main():\n",
    "    checkpoint = load_checkpoint()\n",
    "    processed = checkpoint.get(\"processed\", [])\n",
    "    \n",
    "    # Get all PDB files from the input folder\n",
    "    pdb_files = glob.glob(os.path.join(INPUT_FOLDER, \"*.pdb\"))\n",
    "    print(f\"Found {len(pdb_files)} PDB files in {INPUT_FOLDER}.\")\n",
    "    \n",
    "    for pdb_file in pdb_files:\n",
    "        base_name = os.path.basename(pdb_file)\n",
    "        if base_name in processed:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing {base_name}...\")\n",
    "        mol = Chem.MolFromPDBFile(pdb_file, removeHs=False)\n",
    "        if mol is None:\n",
    "            print(f\"Failed to load molecule from {base_name}.\")\n",
    "            processed.append(base_name)\n",
    "            save_checkpoint({\"processed\": processed})\n",
    "            continue\n",
    "        \n",
    "        mol = Chem.AddHs(mol)\n",
    "        if mol.GetNumConformers() == 0:\n",
    "            AllChem.EmbedMolecule(mol)\n",
    "            AllChem.UFFOptimizeMolecule(mol)\n",
    "        \n",
    "        # Convert molecule to PDBQT\n",
    "        pdbqt_path = os.path.join(TEMP_FOLDER, f\"temp_{base_name}.pdbqt\")\n",
    "        if not convert_to_pdbqt(mol, pdbqt_path):\n",
    "            print(f\"{base_name} failed PDBQT conversion.\")\n",
    "            processed.append(base_name)\n",
    "            save_checkpoint({\"processed\": processed})\n",
    "            continue\n",
    "        \n",
    "        # Dock the molecule\n",
    "        score = run_vina_docking(pdbqt_path)\n",
    "        if score is not None:\n",
    "            print(f\"{base_name}: docking score = {score}\")\n",
    "            if score < AFFINITY_CUTOFF and passes_lipinski_ro5(mol):\n",
    "                # Save accepted compound as an individual SDF file named by the original file (without extension)\n",
    "                out_filename = os.path.join(OUTPUT_FOLDER, f\"{os.path.splitext(base_name)[0]}.sdf\")\n",
    "                writer = Chem.SDWriter(out_filename)\n",
    "                mol.SetProp(\"Docking_Score\", str(score))\n",
    "                writer.write(mol)\n",
    "                writer.close()\n",
    "                print(f\"Accepted {base_name} saved to {out_filename}\")\n",
    "            else:\n",
    "                print(f\"{base_name} did not meet filtering criteria.\")\n",
    "        else:\n",
    "            print(f\"{base_name}: docking failed or no score obtained.\")\n",
    "        \n",
    "        processed.append(base_name)\n",
    "        save_checkpoint({\"processed\": processed})\n",
    "        if os.path.exists(pdbqt_path):\n",
    "            os.remove(pdbqt_path)\n",
    "    \n",
    "    print(\"\\nProcessing complete. Filtered compounds saved in:\", OUTPUT_FOLDER)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db48a76-c440-4841-86a9-4933151f021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "import pandas as pd\n",
    "\n",
    "def convert_sdf_to_smiles(sdf_file, output_csv):\n",
    "    suppl = Chem.SDMolSupplier(sdf_file)\n",
    "    data = []\n",
    "\n",
    "    for mol in suppl:\n",
    "        if mol is not None:\n",
    "            smiles = Chem.MolToSmiles(mol)\n",
    "            compound_id = mol.GetProp(\"_Name\") if mol.HasProp(\"_Name\") else \"Unknown\"\n",
    "            data.append([compound_id, smiles])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"Compound_ID\", \"SMILES\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved {len(df)} compounds to {output_csv}\")\n",
    "\n",
    "convert_sdf_to_smiles(\"cleanedsolu.sdf\", \"fragments_smiles.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3e11f3-79f4-4ddf-a1ac-3d6b8cab1177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "def compute_molecular_features(input_csv, output_csv):\n",
    "    df = pd.read_csv(input_csv)\n",
    "    features = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        mol = Chem.MolFromSmiles(row[\"SMILES\"])\n",
    "        if mol:\n",
    "            mw = Descriptors.MolWt(mol)\n",
    "            hbd = Descriptors.NumHDonors(mol)\n",
    "            hba = Descriptors.NumHAcceptors(mol)\n",
    "            rb = Descriptors.NumRotatableBonds(mol)\n",
    "            logp = Descriptors.MolLogP(mol)\n",
    "\n",
    "            lipinski_pass = (mw <= 500 and hbd <= 5 and hba <= 10 and logp <= 5 and rb <= 10)\n",
    "\n",
    "            features.append([row[\"Compound_ID\"], row[\"SMILES\"], mw, hbd, hba, rb, logp, lipinski_pass])\n",
    "\n",
    "    df_features = pd.DataFrame(features, columns=[\"Compound_ID\", \"SMILES\", \"MW\", \"HBD\", \"HBA\", \"RB\", \"LogP\", \"Lipinski_Pass\"])\n",
    "    df_features.to_csv(output_csv, index=False)\n",
    "    print(f\"Saved molecular features to {output_csv}\")\n",
    "\n",
    "compute_molecular_features(\"fragments_smiles.csv\", \"compound_features_new.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9410dfd-75be-41a0-8970-2b6232c8fb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "\n",
    "# Configuration: File paths for the two datasets\n",
    "KNOWN_CSV     = \"Enamine.csv\"                # Known reported compounds\n",
    "FRAGMENTS_CSV = \"compound_features_new.csv\"  # New fragments to compare\n",
    "OUTPUT_CSV    = \"Enamine_matching_results.csv\"\n",
    "\n",
    "def ensure_id_column(df, id_col_name, prefix):\n",
    "    \"\"\"If id_col_name is missing, add it as prefix_1, prefix_2, ….\"\"\"\n",
    "    if id_col_name not in df.columns:\n",
    "        df[id_col_name] = [f\"{prefix}_{i+1}\" for i in range(len(df))]\n",
    "    return df\n",
    "\n",
    "def find_and_rename_smiles(df):\n",
    "    \"\"\"\n",
    "    Find any column whose name contains 'smile' (case‐insensitive),\n",
    "    strip it, and rename that column to exactly 'SMILES'. Returns True if found.\n",
    "    \"\"\"\n",
    "    # strip leading/trailing whitespace\n",
    "    cols = [c.strip() for c in df.columns]\n",
    "    df.columns = cols\n",
    "\n",
    "    # look for any column containing 'smile'\n",
    "    for c in cols:\n",
    "        if \"smile\" in c.lower():\n",
    "            if c != \"SMILES\":\n",
    "                df.rename(columns={c: \"SMILES\"}, inplace=True)\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_fingerprint(smiles, radius=2, nBits=2048):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    return AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits)\n",
    "\n",
    "def main():\n",
    "    known_df = pd.read_csv(KNOWN_CSV, sep=None, engine='python')\n",
    "    frag_df  = pd.read_csv(FRAGMENTS_CSV)\n",
    "\n",
    "    # robustly find & rename the SMILES column\n",
    "    ok1 = find_and_rename_smiles(known_df)\n",
    "    ok2 = find_and_rename_smiles(frag_df)\n",
    "    if not (ok1 and ok2):\n",
    "        print(\"Error: couldn’t find a column containing 'SMILES' in one of the files.\")\n",
    "        print(\"Known CSV columns:\", known_df.columns.tolist())\n",
    "        print(\"Fragments CSV columns:\", frag_df.columns.tolist())\n",
    "        return\n",
    "\n",
    "    # ensure ID columns\n",
    "    known_df = ensure_id_column(known_df, \"Catalog ID\", \"Known\")\n",
    "    frag_df  = ensure_id_column(frag_df, \"Fragment_ID\", \"Frag\")\n",
    "\n",
    "    # compute fingerprints\n",
    "    print(\"Computing fingerprints for known compounds...\")\n",
    "    known_df[\"Fingerprint\"] = known_df[\"SMILES\"].map(get_fingerprint)\n",
    "    print(\"Computing fingerprints for fragments...\")\n",
    "    frag_df[\"Fingerprint\"]  = frag_df[\"SMILES\"].map(get_fingerprint)\n",
    "\n",
    "    # drop invalid SMILES\n",
    "    known_df = known_df[known_df[\"Fingerprint\"].notnull()].reset_index(drop=True)\n",
    "    frag_df  = frag_df[frag_df[\"Fingerprint\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "    records = []\n",
    "    # for each known compound, find best‐matching fragment\n",
    "    for _, kr in known_df.iterrows():\n",
    "        k_id     = kr[\"Catalog ID\"]\n",
    "        k_smiles = kr[\"SMILES\"]\n",
    "        k_fp     = kr[\"Fingerprint\"]\n",
    "\n",
    "        best_sim      = -1.0\n",
    "        best_frag_id  = None\n",
    "        best_frag_sm  = None\n",
    "\n",
    "        for _, fr in frag_df.iterrows():\n",
    "            f_smiles = fr[\"SMILES\"]\n",
    "            f_fp      = fr[\"Fingerprint\"]\n",
    "            sim       = DataStructs.TanimotoSimilarity(k_fp, f_fp)\n",
    "            if sim > best_sim:\n",
    "                best_sim       = sim\n",
    "                best_frag_id   = fr[\"Fragment_ID\"]\n",
    "                best_frag_sm   = f_smiles\n",
    "\n",
    "        records.append({\n",
    "            \"Known_Compound_ID\":       k_id,\n",
    "            \"Known_SMILES\":            k_smiles,\n",
    "            \"Matched_Fragment_ID\":     best_frag_id,\n",
    "            \"Matched_Fragment_SMILES\": best_frag_sm,\n",
    "            \"Matching_Percentage\":     best_sim * 100\n",
    "        })\n",
    "\n",
    "    out_df = pd.DataFrame(records)\n",
    "    out_df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"Saved matching results to {OUTPUT_CSV}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26615fc-f9ee-48da-95d9-5629bd8e17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from rdkit import Chem\n",
    "\n",
    "# Configuration\n",
    "INPUT_SDF = \"Enamine.sdf\"         # Multi-compound SDF file\n",
    "OUTPUT_DIR = \"Enamine_sdfs\"  # Folder to save individual SDF files\n",
    "PUBCHEM_TIMEOUT = 10             # seconds for HTTP requests\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Helper: find the SDF property key that contains 'catalog'\n",
    "def find_catalog_field(mol):\n",
    "    for prop_name in mol.GetPropNames():\n",
    "        if 'catalog' in prop_name.lower():\n",
    "            return prop_name\n",
    "    return None\n",
    "\n",
    "# Helper: sanitize filename\n",
    "def sanitize_name(name):\n",
    "    name = name.strip()\n",
    "    # replace invalid chars\n",
    "    return re.sub(r\"[^\\w\\-_. ]\", \"_\", name)\n",
    "\n",
    "# PubChem lookup functions\n",
    "def fetch_pubchem_title(identifier):\n",
    "    \"\"\"Try to fetch Title for a compound by name or CID from PubChem.\"\"\"\n",
    "    # Try lookup by name\n",
    "    url = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{identifier}/property/Title/JSON\"\n",
    "    resp = requests.get(url, timeout=PUBCHEM_TIMEOUT)\n",
    "    if resp.status_code == 200:\n",
    "        try:\n",
    "            return resp.json()['PropertyTable']['Properties'][0]['Title']\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Try lookup by CID\n",
    "    url2 = f\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/{identifier}/property/Title/JSON\"\n",
    "    resp2 = requests.get(url2, timeout=PUBCHEM_TIMEOUT)\n",
    "    if resp2.status_code == 200:\n",
    "        try:\n",
    "            return resp2.json()['PropertyTable']['Properties'][0]['Title']\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "# Main splitting logic\n",
    "supplier = Chem.SDMolSupplier(INPUT_SDF, removeHs=False)\n",
    "print(f\"Loaded {len(supplier)} molecules from {INPUT_SDF}.\")\n",
    "\n",
    "for idx, mol in enumerate(supplier, start=1):\n",
    "    if mol is None:\n",
    "        print(f\"Skipping molecule #{idx}: parse error.\")\n",
    "        continue\n",
    "\n",
    "    # Find the catalog field dynamically\n",
    "    cat_field = find_catalog_field(mol)\n",
    "    if cat_field:\n",
    "        identifier = mol.GetProp(cat_field).strip()\n",
    "    else:\n",
    "        identifier = f\"compound_{idx}\"\n",
    "\n",
    "    # Try to fetch a PubChem title\n",
    "    pubchem_name = None\n",
    "    try:\n",
    "        pubchem_name = fetch_pubchem_title(identifier)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: PubChem lookup failed for '{identifier}': {e}\")\n",
    "\n",
    "    # Choose the filename\n",
    "    base = pubchem_name or identifier\n",
    "    base = sanitize_name(base)\n",
    "    out_path = os.path.join(OUTPUT_DIR, f\"{base}.sdf\")\n",
    "\n",
    "    # Write the molecule\n",
    "    writer = Chem.SDWriter(out_path)\n",
    "    writer.write(mol)\n",
    "    writer.close()\n",
    "    print(f\"Saved molecule #{idx} as '{out_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e19500-0578-4d1a-ae25-13cc47adcc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "\n",
    "def check_molecule(path, name):\n",
    "    mol = Chem.MolFromMolFile(path)\n",
    "    print(f\"\\n{name} Analysis:\")\n",
    "    for atom in mol.GetAtoms():\n",
    "        print(f\"Atom {atom.GetIdx()}: {atom.GetSymbol()} | Bonds: {atom.GetDegree()} | Valence: {atom.GetExplicitValence()}\")\n",
    "\n",
    "# Check each component\n",
    "check_molecule(\"xxxxx.sdf\", \"WARHEAD\")\n",
    "check_molecule(\"RNF114.sdf\", \"BINDER\")\n",
    "check_molecule(\"CDK10.sdf\", \"LINKER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7b6478-f607-4330-bb7e-00f5b12cf7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "# Function to load the first molecule from an SDF file.\n",
    "def load_molecule(sdf_filename):\n",
    "    supplier = Chem.SDMolSupplier(sdf_filename)\n",
    "    if not supplier or len(supplier) == 0:\n",
    "        raise IOError(f\"Could not load any molecule from {sdf_filename}\")\n",
    "    mol = supplier[0]\n",
    "    if mol is None:\n",
    "        raise ValueError(f\"Failed to parse molecule from {sdf_filename}\")\n",
    "    return mol\n",
    "\n",
    "# Load molecules from SDF files.\n",
    "warhead = load_molecule(\"RL559.sdf\")\n",
    "linker  = load_molecule(\"AR_Del_388-390.sdf\")\n",
    "binder  = load_molecule(\"cIAP1_0.sdf\")\n",
    "\n",
    "# Define a reaction SMARTS that connects two molecules at their dummy atoms.\n",
    "# The reaction SMARTS \"[*:1][*].[*:2][*]>>[*:1]-[*:2]\" means:\n",
    "#  - Remove a dummy atom from each reactant and form a single bond between the atoms formerly attached.\n",
    "rxn = AllChem.ReactionFromSmarts(\"[*:1][*].[*:2][*]>>[*:1]-[*:2]\")\n",
    "\n",
    "# ---- Step 1: Connect Warhead and Linker ----\n",
    "products = rxn.RunReactants((warhead, linker))\n",
    "if products:\n",
    "    # Take the first product as the intermediate.\n",
    "    intermediate = products[0][0]\n",
    "    # Skip kekulization if needed to avoid aromaticity issues.\n",
    "    Chem.SanitizeMol(intermediate, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL ^ Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "else:\n",
    "    raise ValueError(\"Reaction between warhead and linker failed.\")\n",
    "\n",
    "# ---- Step 2: Connect the Intermediate and Binder ----\n",
    "products_final = rxn.RunReactants((intermediate, binder))\n",
    "if products_final:\n",
    "    final_mol = products_final[0][0]\n",
    "    Chem.SanitizeMol(final_mol, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL ^ Chem.SanitizeFlags.SANITIZE_KEKULIZE)\n",
    "else:\n",
    "    raise ValueError(\"Reaction between intermediate and binder failed.\")\n",
    "\n",
    "# Output the final PROTAC molecule as a SMILES string.\n",
    "final_smiles = Chem.MolToSmiles(final_mol)\n",
    "print(\"Final PROTAC SMILES:\", final_smiles)\n",
    "\n",
    "# Optionally, write the final molecule to an SDF file.\n",
    "writer = Chem.SDWriter(\"final_protac.sdf\")\n",
    "writer.write(final_mol)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9eb15d-8e41-44a0-aaf7-1355a4bfa4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import rdchem\n",
    "\n",
    "# === User parameters ===\n",
    "binder_file   = \"RNF114.sdf\"\n",
    "linker_file   = \"CDK10.sdf\"\n",
    "warhead_dir   = \"warheads\"\n",
    "\n",
    "# Heavy‐atom indices to join (0‐based)\n",
    "binder_idx           = 0\n",
    "linker_binder_idx    = 10\n",
    "linker_warhead_idx   = 6\n",
    "warhead_idx          = 29\n",
    "\n",
    "# === Utility: load, embed, optimize, and cleanup Hs ===\n",
    "def load_and_prep(path):\n",
    "    mol = Chem.SDMolSupplier(path, removeHs=False)[0]\n",
    "    if mol is None:\n",
    "        raise IOError(f\"Could not load '{path}'\")\n",
    "    mol = Chem.AddHs(mol)\n",
    "    AllChem.EmbedMolecule(mol)\n",
    "    AllChem.UFFOptimizeMolecule(mol)\n",
    "    # Remove explicit Hs to avoid valence issues\n",
    "    mol = Chem.RemoveHs(mol)\n",
    "    return mol\n",
    "\n",
    "# 1) Load & prep binder + linker\n",
    "binder = load_and_prep(binder_file)\n",
    "linker = load_and_prep(linker_file)\n",
    "\n",
    "# 2) Build base (binder + linker)\n",
    "base_rw = Chem.RWMol(binder)\n",
    "linker_map = {}\n",
    "for atom in linker.GetAtoms():\n",
    "    linker_map[atom.GetIdx()] = base_rw.AddAtom(atom)\n",
    "for bond in linker.GetBonds():\n",
    "    a1 = linker_map[bond.GetBeginAtomIdx()]\n",
    "    a2 = linker_map[bond.GetEndAtomIdx()]\n",
    "    base_rw.AddBond(a1, a2, bond.GetBondType())\n",
    "# binder→linker bond\n",
    "base_rw.AddBond(binder_idx, linker_map[linker_binder_idx], rdchem.BondType.SINGLE)\n",
    "\n",
    "# Debug sanitize with detailed atom info if error\n",
    "try:\n",
    "    base_mol = base_rw.GetMol()\n",
    "    Chem.SanitizeMol(base_mol)\n",
    "except Chem.AtomValenceException as e:\n",
    "    # Find offending atom(s)\n",
    "    for atom in base_rw.GetMol().GetAtoms():\n",
    "        if atom.GetExplicitValence() > Chem.GetPeriodicTable().GetDefaultValence(atom.GetAtomicNum()):\n",
    "            print(f\"Valence error at atom idx {atom.GetIdx()} ({atom.GetSymbol()}): \"\n",
    "                  f\"explicit valence {atom.GetExplicitValence()} > allowed.\")\n",
    "    raise\n",
    "\n",
    "# Prepare names & SMILES\n",
    "binder_name  = os.path.splitext(os.path.basename(binder_file))[0]\n",
    "linker_name  = os.path.splitext(os.path.basename(linker_file))[0]\n",
    "binder_smile = Chem.MolToSmiles(binder)\n",
    "linker_smile = Chem.MolToSmiles(linker)\n",
    "\n",
    "# 3) Loop over warheads\n",
    "rows = []\n",
    "for fname in sorted(os.listdir(warhead_dir)):\n",
    "    if not fname.lower().endswith(\".sdf\"):\n",
    "        continue\n",
    "    warpath = os.path.join(warhead_dir, fname)\n",
    "    warname = os.path.splitext(fname)[0]\n",
    "\n",
    "    try:\n",
    "        # load & prep warhead\n",
    "        war = load_and_prep(warpath)\n",
    "\n",
    "        # combine base + warhead\n",
    "        rw = Chem.RWMol(base_mol)\n",
    "        war_map = {}\n",
    "        for atom in war.GetAtoms():\n",
    "            war_map[atom.GetIdx()] = rw.AddAtom(atom)\n",
    "        for bond in war.GetBonds():\n",
    "            a1 = war_map[bond.GetBeginAtomIdx()]\n",
    "            a2 = war_map[bond.GetEndAtomIdx()]\n",
    "            rw.AddBond(a1, a2, bond.GetBondType())\n",
    "        # linker→warhead bond\n",
    "        rw.AddBond(linker_map[linker_warhead_idx],\n",
    "                   war_map[warhead_idx],\n",
    "                   rdchem.BondType.SINGLE)\n",
    "\n",
    "        # sanitize and optimize\n",
    "        protac = rw.GetMol()\n",
    "        Chem.SanitizeMol(protac)\n",
    "        AllChem.EmbedMolecule(protac)\n",
    "        AllChem.UFFOptimizeMolecule(protac)\n",
    "\n",
    "        # write out SDF\n",
    "        protac_name = f\"{warname}_protac\"\n",
    "        sdf_out     = os.path.join(warhead_dir, f\"{protac_name}.sdf\")\n",
    "        w = Chem.SDWriter(sdf_out)\n",
    "        w.write(protac)\n",
    "        w.close()\n",
    "\n",
    "        # record CSV\n",
    "        rows.append({\n",
    "            \"warhead_name\":   warname,\n",
    "            \"warhead_smiles\": Chem.MolToSmiles(war),\n",
    "            \"linker_name\":    linker_name,\n",
    "            \"linker_smiles\":  linker_smile,\n",
    "            \"binder_name\":    binder_name,\n",
    "            \"binder_smiles\":  binder_smile,\n",
    "            \"protac_name\":    protac_name,\n",
    "            \"protac_smiles\":  Chem.MolToSmiles(protac)\n",
    "        })\n",
    "        print(f\"✅ Built PROTAC for {warname}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipping '{fname}' due to error: {e}\")\n",
    "\n",
    "# 4) Write summary CSV\n",
    "csv_path = os.path.join(warhead_dir, \"protac_summary.csv\")\n",
    "with open(csv_path, \"w\", newline=\"\") as csvfile:\n",
    "    fieldnames = [\n",
    "        \"warhead_name\",\"warhead_smiles\",\n",
    "        \"linker_name\",\"linker_smiles\",\n",
    "        \"binder_name\",\"binder_smiles\",\n",
    "        \"protac_name\",\"protac_smiles\"\n",
    "    ]\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"\\n✅ Finished. {len(rows)} PROTAC(s) generated. CSV summary at '{csv_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77342ea2-c66c-4a09-8a21-5cf836b40262",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os, glob, csv, re\n",
    "\n",
    "LOG_PATTERN = \"*.pdbqt_log.log\"\n",
    "OUTPUT_CSV  = \"vina_mode1_scores.csv\"\n",
    "\n",
    "def parse_mode1_affinity(logpath):\n",
    "    with open(logpath) as f:\n",
    "        lines = f.readlines()\n",
    "    # header‑based parse\n",
    "    for i, L in enumerate(lines):\n",
    "        if re.match(r\"\\s*mode\\s*\\|\\s*affinity\", L, re.IGNORECASE):\n",
    "            idx = i + 2\n",
    "            if idx < len(lines):\n",
    "                parts = lines[idx].split()\n",
    "                if parts and parts[0] == \"1\":\n",
    "                    try:\n",
    "                        return float(parts[1])\n",
    "                    except:\n",
    "                        pass\n",
    "            break\n",
    "    # fallback scan\n",
    "    for L in lines:\n",
    "        parts = L.split()\n",
    "        if len(parts) >= 2 and parts[0] == \"1\":\n",
    "            try:\n",
    "                return float(parts[1])\n",
    "            except:\n",
    "                continue\n",
    "    return None\n",
    "\n",
    "def main():\n",
    "    logs = glob.glob(LOG_PATTERN)\n",
    "    records = []\n",
    "    for log in logs:\n",
    "        base = os.path.basename(log).split(\".pdbqt\")[0]\n",
    "        score = parse_mode1_affinity(log)\n",
    "        if score is None:\n",
    "            print(f\"[WARN] Could not parse mode‑1 from `{log}`\")\n",
    "        else:\n",
    "            print(f\"{base} → {score:.2f}\")\n",
    "        records.append((base, score))\n",
    "    # write CSV\n",
    "    with open(OUTPUT_CSV, \"w\", newline=\"\") as outf:\n",
    "        w = csv.writer(outf)\n",
    "        w.writerow([\"Compound\", \"Mode1_Affinity\"])\n",
    "        for comp, aff in records:\n",
    "            w.writerow([comp, aff if aff is not None else \"\"])\n",
    "    print(f\"\\nWritten {len(records)} entries to `{OUTPUT_CSV}`\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f60393-f673-4f09-8d31-1b6ab6ed9c66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
